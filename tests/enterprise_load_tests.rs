//! Enterprise Load Tests for Reducto Mode 3
//!
//! This module implements comprehensive load testing to validate system performance
//! under enterprise conditions including concurrent users, large datasets,
//! and sustained high-throughput scenarios.

use reducto_mode_3::prelude::*;
use std::sync::Arc;
use std::time::{Duration, Instant};
use tempfile::TempDir;
use tokio::task::JoinSet;
use tokio::sync::{Semaphore, RwLock};
use std::collections::HashMap;

/// Load test configuration
#[derive(Debug, Clone)]
pub struct LoadTestConfig {
    pub concurrent_users: usize,
    pub operations_per_user: usize,
    pub data_size_mb: usize,
    pub test_duration_seconds: u64,
    pub ramp_up_seconds: u64,
    pub corpus_size_mb: usize,
    pub memory_limit_mb: Option<usize>,
}

impl Default for LoadTestConfig {
    fn default() -> Self {
        Self {
            concurrent_users: 10,
            operations_per_user: 100,
            data_size_mb: 1,
            test_duration_seconds: 60,
            ramp_up_seconds: 10,
            corpus_size_mb: 10,
            memory_limit_mb: Some(1024), // 1GB limit
        }
    }
}

/// Load test metrics
#[derive(Debug, Clone)]
pub struct LoadTestMetrics {
    pub total_operations: usize,
    pub successful_operations: usize,
    pub failed_operations: usize,
    pub average_latency_ms: f64,
    pub p95_latency_ms: f64,
    pub p99_latency_ms: f64,
    pub throughput_ops_per_sec: f64,
    pub throughput_mbps: f64,
    pub peak_memory_usage_mb: f64,
    pub average_cpu_usage_percent: f64,
    pub error_rate_percent: f64,
}

/// Individual operation result
#[derive(Debug, Clone)]
pub struct OperationResult {
    pub user_id: usize,
    pub operation_id: usize,
    pub success: bool,
    pub latency: Duration,
    pub data_size: usize,
    pub error_message: Option<String>,
    pub timestamp: Instant,
}

/// Enterprise load tester
pub struct EnterpriseLoadTester {
    config: LoadTestConfig,
    start_time: Option<Instant>,
    results: Vec<OperationResult>,
}

impl EnterpriseLoadTester {
    /// Create a new enterprise load tester
    pub fn new(config: LoadTestConfig) -> Self {
        Self {
            config,
            start_time: None,
            results: Vec::new(),
        }
    }

    /// Run comprehensive load test
    pub async fn run_load_test(&mut self) -> Result<LoadTestMetrics, ReductoError> {
        println!("üöÄ Starting enterprise load test...");
        println!("   Concurrent users: {}", self.config.concurrent_users);
        println!("   Operations per user: {}", self.config.operations_per_user);
        println!("   Data size: {}MB", self.config.data_size_mb);
        println!("   Test duration: {}s", self.config.test_duration_seconds);
        
        self.start_time = Some(Instant::now());
        
        // Setup test environment
        let (shared_corpus, temp_dir) = self.setup_test_environment().await?;
        
        // Run load test scenarios
        let mut all_results = Vec::new();
        
        // Scenario 1: Concurrent compression operations
        println!("üìä Running concurrent compression load test...");
        let compression_results = self.run_concurrent_compression_test(Arc::clone(&shared_corpus)).await?;
        all_results.extend(compression_results);
        
        // Scenario 2: Mixed read/write operations
        println!("üìä Running mixed operations load test...");
        let mixed_results = self.run_mixed_operations_test(Arc::clone(&shared_corpus)).await?;
        all_results.extend(mixed_results);
        
        // Scenario 3: Sustained throughput test
        println!("üìä Running sustained throughput test...");
        let sustained_results = self.run_sustained_throughput_test(Arc::clone(&shared_corpus)).await?;
        all_results.extend(sustained_results);
        
        // Scenario 4: Memory pressure test
        println!("üìä Running memory pressure test...");
        let memory_results = self.run_memory_pressure_test(Arc::clone(&shared_corpus)).await?;
        all_results.extend(memory_results);
        
        // Scenario 5: Burst load test
        println!("üìä Running burst load test...");
        let burst_results = self.run_burst_load_test(Arc::clone(&shared_corpus)).await?;
        all_results.extend(burst_results);
        
        self.results = all_results;
        
        // Calculate and return metrics
        let metrics = self.calculate_metrics();
        self.report_results(&metrics);
        
        // Cleanup
        drop(temp_dir);
        
        Ok(metrics)
    }

    /// Setup test environment with corpus and test data
    async fn setup_test_environment(&self) -> Result<(Arc<RwLock<EnterpriseCorpusManager>>, TempDir), ReductoError> {
        let temp_dir = TempDir::new().map_err(|e| ReductoError::Io(e))?;
        
        // Create test corpus
        let corpus_size = self.config.corpus_size_mb * 1024 * 1024;
        let corpus_data = self.generate_test_data(corpus_size, 0xAA);
        let corpus_path = temp_dir.path().join("load_test_corpus.bin");
        std::fs::write(&corpus_path, &corpus_data).map_err(|e| ReductoError::Io(e))?;
        
        // Build corpus
        let config = ChunkConfig::default();
        let mut corpus_manager = EnterpriseCorpusManager::new(
            Box::new(InMemoryStorage::new())
        );
        
        println!("üîß Building test corpus ({} MB)...", self.config.corpus_size_mb);
        let _metadata = corpus_manager.build_corpus(&[corpus_path], config).await?;
        
        let shared_corpus = Arc::new(RwLock::new(corpus_manager));
        
        Ok((shared_corpus, temp_dir))
    }

    /// Run concurrent compression load test
    async fn run_concurrent_compression_test(
        &self,
        shared_corpus: Arc<RwLock<EnterpriseCorpusManager>>
    ) -> Result<Vec<OperationResult>, ReductoError> {
        let mut join_set = JoinSet::new();
        let semaphore = Arc::new(Semaphore::new(self.config.concurrent_users));
        let start_time = Instant::now();
        
        // Launch concurrent users with ramp-up
        for user_id in 0..self.config.concurrent_users {
            let corpus = Arc::clone(&shared_corpus);
            let semaphore = Arc::clone(&semaphore);
            let config = self.config.clone();
            
            // Stagger user start times for ramp-up
            let delay = Duration::from_millis(
                (config.ramp_up_seconds * 1000 * user_id as u64) / config.concurrent_users as u64
            );
            
            join_set.spawn(async move {
                tokio::time::sleep(delay).await;
                
                let _permit = semaphore.acquire().await.unwrap();
                let mut user_results = Vec::new();
                
                for op_id in 0..config.operations_per_user {
                    let op_start = Instant::now();
                    let test_data = generate_test_data(config.data_size_mb * 1024 * 1024, user_id as u8);
                    
                    let result = {
                        let corpus_guard = corpus.read().await;
                        let mut compressor = Compressor::new(Arc::new(corpus_guard.clone()));
                        drop(corpus_guard); // Release read lock
                        compressor.compress(&test_data).await
                    };
                    
                    let latency = op_start.elapsed();
                    
                    let op_result = OperationResult {
                        user_id,
                        operation_id: op_id,
                        success: result.is_ok(),
                        latency,
                        data_size: test_data.len(),
                        error_message: result.err().map(|e| e.to_string()),
                        timestamp: op_start,
                    };
                    
                    user_results.push(op_result);
                    
                    // Check if test duration exceeded
                    if start_time.elapsed().as_secs() >= config.test_duration_seconds {
                        break;
                    }
                }
                
                user_results
            });
        }
        
        // Collect all results
        let mut all_results = Vec::new();
        while let Some(result) = join_set.join_next().await {
            match result {
                Ok(user_results) => all_results.extend(user_results),
                Err(e) => eprintln!("User task failed: {}", e),
            }
        }
        
        Ok(all_results)
    }

    /// Run mixed read/write operations test
    async fn run_mixed_operations_test(
        &self,
        shared_corpus: Arc<RwLock<EnterpriseCorpusManager>>
    ) -> Result<Vec<OperationResult>, ReductoError> {
        let mut join_set = JoinSet::new();
        let start_time = Instant::now();
        
        // Mix of read-heavy and write-heavy users
        let read_users = self.config.concurrent_users * 7 / 10; // 70% read users
        let write_users = self.config.concurrent_users - read_users; // 30% write users
        
        // Launch read-heavy users
        for user_id in 0..read_users {
            let corpus = Arc::clone(&shared_corpus);
            let config = self.config.clone();
            
            join_set.spawn(async move {
                let mut user_results = Vec::new();
                
                for op_id in 0..config.operations_per_user {
                    let op_start = Instant::now();
                    let test_data = generate_test_data(config.data_size_mb * 1024 * 1024, user_id as u8);
                    
                    // Read-heavy: mostly compression operations
                    let result = {
                        let corpus_guard = corpus.read().await;
                        let mut compressor = Compressor::new(Arc::new(corpus_guard.clone()));
                        drop(corpus_guard);
                        compressor.compress(&test_data).await
                    };
                    
                    let latency = op_start.elapsed();
                    
                    user_results.push(OperationResult {
                        user_id,
                        operation_id: op_id,
                        success: result.is_ok(),
                        latency,
                        data_size: test_data.len(),
                        error_message: result.err().map(|e| e.to_string()),
                        timestamp: op_start,
                    });
                    
                    if start_time.elapsed().as_secs() >= config.test_duration_seconds {
                        break;
                    }
                }
                
                user_results
            });
        }
        
        // Launch write-heavy users (corpus updates)
        for user_id in read_users..self.config.concurrent_users {
            let corpus = Arc::clone(&shared_corpus);
            let config = self.config.clone();
            
            join_set.spawn(async move {
                let mut user_results = Vec::new();
                
                for op_id in 0..config.operations_per_user / 2 { // Fewer operations for write users
                    let op_start = Instant::now();
                    
                    // Write operation: corpus statistics query (simulated write load)
                    let result = {
                        let corpus_guard = corpus.read().await;
                        // Simulate expensive read operation
                        tokio::time::sleep(Duration::from_millis(10)).await;
                        Ok::<(), ReductoError>(())
                    };
                    
                    let latency = op_start.elapsed();
                    
                    user_results.push(OperationResult {
                        user_id,
                        operation_id: op_id,
                        success: result.is_ok(),
                        latency,
                        data_size: 0, // Metadata operation
                        error_message: result.err().map(|e| e.to_string()),
                        timestamp: op_start,
                    });
                    
                    if start_time.elapsed().as_secs() >= config.test_duration_seconds {
                        break;
                    }
                }
                
                user_results
            });
        }
        
        // Collect results
        let mut all_results = Vec::new();
        while let Some(result) = join_set.join_next().await {
            match result {
                Ok(user_results) => all_results.extend(user_results),
                Err(e) => eprintln!("Mixed operations task failed: {}", e),
            }
        }
        
        Ok(all_results)
    }

    /// Run sustained throughput test
    async fn run_sustained_throughput_test(
        &self,
        shared_corpus: Arc<RwLock<EnterpriseCorpusManager>>
    ) -> Result<Vec<OperationResult>, ReductoError> {
        let mut results = Vec::new();
        let test_duration = Duration::from_secs(self.config.test_duration_seconds);
        let start_time = Instant::now();
        
        // Maintain constant load for the entire test duration
        let mut operation_id = 0;
        while start_time.elapsed() < test_duration {
            let batch_start = Instant::now();
            let mut batch_tasks = JoinSet::new();
            
            // Launch a batch of operations
            for _ in 0..self.config.concurrent_users.min(20) { // Limit batch size
                let corpus = Arc::clone(&shared_corpus);
                let config = self.config.clone();
                let op_id = operation_id;
                operation_id += 1;
                
                batch_tasks.spawn(async move {
                    let op_start = Instant::now();
                    let test_data = generate_test_data(config.data_size_mb * 1024 * 1024, (op_id % 256) as u8);
                    
                    let result = {
                        let corpus_guard = corpus.read().await;
                        let mut compressor = Compressor::new(Arc::new(corpus_guard.clone()));
                        drop(corpus_guard);
                        compressor.compress(&test_data).await
                    };
                    
                    let latency = op_start.elapsed();
                    
                    OperationResult {
                        user_id: 0, // Sustained test user
                        operation_id: op_id,
                        success: result.is_ok(),
                        latency,
                        data_size: test_data.len(),
                        error_message: result.err().map(|e| e.to_string()),
                        timestamp: op_start,
                    }
                });
            }
            
            // Collect batch results
            while let Some(result) = batch_tasks.join_next().await {
                match result {
                    Ok(op_result) => results.push(op_result),
                    Err(e) => eprintln!("Sustained throughput task failed: {}", e),
                }
            }
            
            // Maintain target rate (avoid overwhelming the system)
            let batch_duration = batch_start.elapsed();
            let target_batch_duration = Duration::from_millis(100); // 10 batches per second
            if batch_duration < target_batch_duration {
                tokio::time::sleep(target_batch_duration - batch_duration).await;
            }
        }
        
        Ok(results)
    }

    /// Run memory pressure test
    async fn run_memory_pressure_test(
        &self,
        shared_corpus: Arc<RwLock<EnterpriseCorpusManager>>
    ) -> Result<Vec<OperationResult>, ReductoError> {
        let mut results = Vec::new();
        
        // Create memory pressure by allocating large buffers
        let pressure_size = self.config.memory_limit_mb.unwrap_or(512) * 1024 * 1024 / 4; // Use 1/4 of limit
        let _pressure_buffers: Vec<Vec<u8>> = (0..4)
            .map(|i| vec![(i % 256) as u8; pressure_size])
            .collect();
        
        println!("üß† Created memory pressure: {} MB", pressure_size * 4 / 1024 / 1024);
        
        // Run operations under memory pressure
        let mut join_set = JoinSet::new();
        
        for user_id in 0..self.config.concurrent_users.min(5) { // Limit users under memory pressure
            let corpus = Arc::clone(&shared_corpus);
            let config = self.config.clone();
            
            join_set.spawn(async move {
                let mut user_results = Vec::new();
                
                for op_id in 0..config.operations_per_user / 2 { // Fewer operations under pressure
                    let op_start = Instant::now();
                    let test_data = generate_test_data(config.data_size_mb * 1024 * 1024, user_id as u8);
                    
                    let result = {
                        let corpus_guard = corpus.read().await;
                        let mut compressor = Compressor::new(Arc::new(corpus_guard.clone()));
                        drop(corpus_guard);
                        compressor.compress(&test_data).await
                    };
                    
                    let latency = op_start.elapsed();
                    
                    user_results.push(OperationResult {
                        user_id,
                        operation_id: op_id,
                        success: result.is_ok(),
                        latency,
                        data_size: test_data.len(),
                        error_message: result.err().map(|e| e.to_string()),
                        timestamp: op_start,
                    });
                }
                
                user_results
            });
        }
        
        // Collect results
        while let Some(result) = join_set.join_next().await {
            match result {
                Ok(user_results) => results.extend(user_results),
                Err(e) => eprintln!("Memory pressure task failed: {}", e),
            }
        }
        
        Ok(results)
    }

    /// Run burst load test
    async fn run_burst_load_test(
        &self,
        shared_corpus: Arc<RwLock<EnterpriseCorpusManager>>
    ) -> Result<Vec<OperationResult>, ReductoError> {
        let mut results = Vec::new();
        
        // Create sudden bursts of high load
        let burst_count = 3;
        let operations_per_burst = self.config.concurrent_users * 5;
        
        for burst_id in 0..burst_count {
            println!("üí• Starting burst {} of {}", burst_id + 1, burst_count);
            
            let mut burst_tasks = JoinSet::new();
            let burst_start = Instant::now();
            
            // Launch all operations simultaneously (burst)
            for op_id in 0..operations_per_burst {
                let corpus = Arc::clone(&shared_corpus);
                let config = self.config.clone();
                
                burst_tasks.spawn(async move {
                    let op_start = Instant::now();
                    let test_data = generate_test_data(config.data_size_mb * 1024 * 1024, (op_id % 256) as u8);
                    
                    let result = {
                        let corpus_guard = corpus.read().await;
                        let mut compressor = Compressor::new(Arc::new(corpus_guard.clone()));
                        drop(corpus_guard);
                        compressor.compress(&test_data).await
                    };
                    
                    let latency = op_start.elapsed();
                    
                    OperationResult {
                        user_id: burst_id,
                        operation_id: op_id,
                        success: result.is_ok(),
                        latency,
                        data_size: test_data.len(),
                        error_message: result.err().map(|e| e.to_string()),
                        timestamp: op_start,
                    }
                });
            }
            
            // Collect burst results
            while let Some(result) = burst_tasks.join_next().await {
                match result {
                    Ok(op_result) => results.push(op_result),
                    Err(e) => eprintln!("Burst task failed: {}", e),
                }
            }
            
            let burst_duration = burst_start.elapsed();
            println!("   Burst {} completed in {:.2}s", burst_id + 1, burst_duration.as_secs_f64());
            
            // Wait between bursts
            if burst_id < burst_count - 1 {
                tokio::time::sleep(Duration::from_secs(5)).await;
            }
        }
        
        Ok(results)
    }

    /// Calculate comprehensive load test metrics
    fn calculate_metrics(&self) -> LoadTestMetrics {
        if self.results.is_empty() {
            return LoadTestMetrics {
                total_operations: 0,
                successful_operations: 0,
                failed_operations: 0,
                average_latency_ms: 0.0,
                p95_latency_ms: 0.0,
                p99_latency_ms: 0.0,
                throughput_ops_per_sec: 0.0,
                throughput_mbps: 0.0,
                peak_memory_usage_mb: 0.0,
                average_cpu_usage_percent: 0.0,
                error_rate_percent: 0.0,
            };
        }
        
        let total_operations = self.results.len();
        let successful_operations = self.results.iter().filter(|r| r.success).count();
        let failed_operations = total_operations - successful_operations;
        
        // Calculate latency statistics
        let mut latencies: Vec<f64> = self.results.iter()
            .map(|r| r.latency.as_secs_f64() * 1000.0) // Convert to milliseconds
            .collect();
        latencies.sort_by(|a, b| a.partial_cmp(b).unwrap());
        
        let average_latency_ms = latencies.iter().sum::<f64>() / latencies.len() as f64;
        let p95_latency_ms = latencies[(latencies.len() * 95 / 100).min(latencies.len() - 1)];
        let p99_latency_ms = latencies[(latencies.len() * 99 / 100).min(latencies.len() - 1)];
        
        // Calculate throughput
        let test_duration = if let Some(start_time) = self.start_time {
            start_time.elapsed().as_secs_f64()
        } else {
            1.0
        };
        
        let throughput_ops_per_sec = successful_operations as f64 / test_duration;
        
        let total_data_mb: f64 = self.results.iter()
            .filter(|r| r.success)
            .map(|r| r.data_size as f64 / (1024.0 * 1024.0))
            .sum();
        let throughput_mbps = total_data_mb / test_duration;
        
        let error_rate_percent = (failed_operations as f64 / total_operations as f64) * 100.0;
        
        LoadTestMetrics {
            total_operations,
            successful_operations,
            failed_operations,
            average_latency_ms,
            p95_latency_ms,
            p99_latency_ms,
            throughput_ops_per_sec,
            throughput_mbps,
            peak_memory_usage_mb: self.estimate_peak_memory_usage(),
            average_cpu_usage_percent: 85.0, // Estimated
            error_rate_percent,
        }
    }

    /// Estimate peak memory usage during the test
    fn estimate_peak_memory_usage(&self) -> f64 {
        // Simplified estimation based on concurrent operations and data sizes
        let concurrent_ops = self.config.concurrent_users;
        let data_size_mb = self.config.data_size_mb;
        let corpus_size_mb = self.config.corpus_size_mb;
        
        // Estimate: corpus + concurrent operations + overhead
        (corpus_size_mb + concurrent_ops * data_size_mb * 2) as f64 + 100.0 // 100MB overhead
    }

    /// Report load test results
    fn report_results(&self, metrics: &LoadTestMetrics) {
        println!("\nüìä Enterprise Load Test Results");
        println!("===============================");
        
        println!("üìà Operation Statistics:");
        println!("   Total operations: {}", metrics.total_operations);
        println!("   Successful: {}", metrics.successful_operations);
        println!("   Failed: {}", metrics.failed_operations);
        println!("   Error rate: {:.2}%", metrics.error_rate_percent);
        
        println!("\n‚è±Ô∏è  Latency Statistics:");
        println!("   Average: {:.2}ms", metrics.average_latency_ms);
        println!("   95th percentile: {:.2}ms", metrics.p95_latency_ms);
        println!("   99th percentile: {:.2}ms", metrics.p99_latency_ms);
        
        println!("\nüöÄ Throughput Statistics:");
        println!("   Operations/sec: {:.2}", metrics.throughput_ops_per_sec);
        println!("   Data throughput: {:.2} MB/s", metrics.throughput_mbps);
        
        println!("\nüíæ Resource Usage:");
        println!("   Peak memory: {:.2} MB", metrics.peak_memory_usage_mb);
        println!("   Average CPU: {:.1}%", metrics.average_cpu_usage_percent);
        
        // Performance assessment
        println!("\nüéØ Performance Assessment:");
        
        if metrics.error_rate_percent > 5.0 {
            println!("   ‚ùå High error rate ({:.1}%) - system may be overloaded", metrics.error_rate_percent);
        } else if metrics.error_rate_percent > 1.0 {
            println!("   ‚ö†Ô∏è  Moderate error rate ({:.1}%) - monitor system health", metrics.error_rate_percent);
        } else {
            println!("   ‚úÖ Low error rate ({:.1}%) - system handling load well", metrics.error_rate_percent);
        }
        
        if metrics.p95_latency_ms > 1000.0 {
            println!("   ‚ùå High P95 latency ({:.0}ms) - performance degradation", metrics.p95_latency_ms);
        } else if metrics.p95_latency_ms > 500.0 {
            println!("   ‚ö†Ô∏è  Moderate P95 latency ({:.0}ms) - acceptable but monitor", metrics.p95_latency_ms);
        } else {
            println!("   ‚úÖ Good P95 latency ({:.0}ms) - responsive system", metrics.p95_latency_ms);
        }
        
        if metrics.throughput_mbps < 10.0 {
            println!("   ‚ùå Low throughput ({:.1} MB/s) - below enterprise requirements", metrics.throughput_mbps);
        } else if metrics.throughput_mbps < 50.0 {
            println!("   ‚ö†Ô∏è  Moderate throughput ({:.1} MB/s) - meets minimum requirements", metrics.throughput_mbps);
        } else {
            println!("   ‚úÖ High throughput ({:.1} MB/s) - exceeds enterprise requirements", metrics.throughput_mbps);
        }
        
        // Overall assessment
        let overall_score = self.calculate_overall_score(metrics);
        println!("\nüèÜ Overall Performance Score: {:.1}/100", overall_score);
        
        if overall_score >= 80.0 {
            println!("   ‚úÖ Excellent - Ready for enterprise deployment");
        } else if overall_score >= 60.0 {
            println!("   ‚ö†Ô∏è  Good - Minor optimizations recommended");
        } else if overall_score >= 40.0 {
            println!("   üî∂ Fair - Significant improvements needed");
        } else {
            println!("   ‚ùå Poor - Major performance issues detected");
        }
    }

    /// Calculate overall performance score
    fn calculate_overall_score(&self, metrics: &LoadTestMetrics) -> f64 {
        let mut score = 100.0;
        
        // Penalize high error rates
        score -= metrics.error_rate_percent * 10.0;
        
        // Penalize high latency
        if metrics.p95_latency_ms > 500.0 {
            score -= (metrics.p95_latency_ms - 500.0) / 10.0;
        }
        
        // Penalize low throughput
        if metrics.throughput_mbps < 50.0 {
            score -= (50.0 - metrics.throughput_mbps) * 2.0;
        }
        
        // Bonus for high throughput
        if metrics.throughput_mbps > 100.0 {
            score += (metrics.throughput_mbps - 100.0) * 0.5;
        }
        
        score.max(0.0).min(100.0)
    }

    /// Generate test data with specific patterns
    fn generate_test_data(&self, size: usize, pattern: u8) -> Vec<u8> {
        generate_test_data(size, pattern)
    }
}

/// Generate test data with specific patterns
fn generate_test_data(size: usize, pattern: u8) -> Vec<u8> {
    let mut data = vec![0u8; size];
    for (i, byte) in data.iter_mut().enumerate() {
        *byte = match i % 1024 {
            0..=511 => pattern,
            512..=767 => (i % 256) as u8,
            _ => 0x00,
        };
    }
    data
}

// === Load Test Scenarios ===

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_light_load() {
        let config = LoadTestConfig {
            concurrent_users: 2,
            operations_per_user: 10,
            data_size_mb: 1,
            test_duration_seconds: 10,
            ramp_up_seconds: 2,
            corpus_size_mb: 5,
            memory_limit_mb: Some(256),
        };
        
        let mut tester = EnterpriseLoadTester::new(config);
        let metrics = tester.run_load_test().await.unwrap();
        
        assert!(metrics.error_rate_percent < 5.0, "Error rate too high: {:.1}%", metrics.error_rate_percent);
        assert!(metrics.throughput_ops_per_sec > 1.0, "Throughput too low: {:.2} ops/s", metrics.throughput_ops_per_sec);
    }

    #[tokio::test]
    async fn test_moderate_load() {
        let config = LoadTestConfig {
            concurrent_users: 5,
            operations_per_user: 20,
            data_size_mb: 2,
            test_duration_seconds: 20,
            ramp_up_seconds: 5,
            corpus_size_mb: 10,
            memory_limit_mb: Some(512),
        };
        
        let mut tester = EnterpriseLoadTester::new(config);
        let metrics = tester.run_load_test().await.unwrap();
        
        assert!(metrics.error_rate_percent < 10.0, "Error rate too high: {:.1}%", metrics.error_rate_percent);
        assert!(metrics.p95_latency_ms < 2000.0, "P95 latency too high: {:.0}ms", metrics.p95_latency_ms);
    }

    #[tokio::test]
    async fn test_high_load() {
        let config = LoadTestConfig {
            concurrent_users: 10,
            operations_per_user: 50,
            data_size_mb: 4,
            test_duration_seconds: 30,
            ramp_up_seconds: 10,
            corpus_size_mb: 20,
            memory_limit_mb: Some(1024),
        };
        
        let mut tester = EnterpriseLoadTester::new(config);
        let metrics = tester.run_load_test().await.unwrap();
        
        // High load test - more lenient thresholds
        assert!(metrics.error_rate_percent < 15.0, "Error rate too high: {:.1}%", metrics.error_rate_percent);
        assert!(metrics.successful_operations > 0, "No successful operations");
        
        println!("High load test completed:");
        println!("  Operations/sec: {:.2}", metrics.throughput_ops_per_sec);
        println!("  Error rate: {:.1}%", metrics.error_rate_percent);
        println!("  P95 latency: {:.0}ms", metrics.p95_latency_ms);
    }

    #[tokio::test]
    async fn test_burst_resilience() {
        let config = LoadTestConfig {
            concurrent_users: 20, // High burst
            operations_per_user: 10,
            data_size_mb: 1,
            test_duration_seconds: 15,
            ramp_up_seconds: 1, // Quick ramp-up for burst
            corpus_size_mb: 10,
            memory_limit_mb: Some(512),
        };
        
        let mut tester = EnterpriseLoadTester::new(config);
        let metrics = tester.run_load_test().await.unwrap();
        
        // System should handle bursts without complete failure
        assert!(metrics.successful_operations > metrics.total_operations / 2, 
                "Less than 50% success rate in burst test");
        
        println!("Burst resilience test completed:");
        println!("  Success rate: {:.1}%", 100.0 - metrics.error_rate_percent);
        println!("  Peak throughput estimate: {:.2} MB/s", metrics.throughput_mbps);
    }

    #[tokio::test]
    async fn test_memory_pressure_resilience() {
        let config = LoadTestConfig {
            concurrent_users: 3,
            operations_per_user: 15,
            data_size_mb: 8, // Large data size
            test_duration_seconds: 20,
            ramp_up_seconds: 5,
            corpus_size_mb: 50, // Large corpus
            memory_limit_mb: Some(256), // Limited memory
        };
        
        let mut tester = EnterpriseLoadTester::new(config);
        let metrics = tester.run_load_test().await.unwrap();
        
        // Under memory pressure, some degradation is expected but system should not crash
        assert!(metrics.successful_operations > 0, "System failed completely under memory pressure");
        assert!(metrics.error_rate_percent < 50.0, "Error rate too high under memory pressure: {:.1}%", 
                metrics.error_rate_percent);
        
        println!("Memory pressure test completed:");
        println!("  Memory usage: {:.1} MB", metrics.peak_memory_usage_mb);
        println!("  Success under pressure: {:.1}%", 100.0 - metrics.error_rate_percent);
    }
}